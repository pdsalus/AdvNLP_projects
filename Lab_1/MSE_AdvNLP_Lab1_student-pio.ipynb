{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logo/0x150/1643104191/logo-mse.png)\n",
    "\n",
    "# AdvNLP Lab 1: Text Tokenization\n",
    "\n",
    "\n",
    "The goal of this lab is to perform tokenization of texts using the [NLTK](http://www.nltk.org/) toolkit or using BPE from [SentencePiece](https://github.com/google/sentencepiece).  You will use the environment that you set up following the instructions of the introductory Jupyter notebook.  \n",
    "\n",
    "You will use NLTK functions to get texts from the web and segment (split) them into sentences and words (also called *tokens*).  You will experiment with extracting statistics about the frequencies of tokens, and compare statistics for a novel and an undeciphered manuscript. \n",
    "\n",
    "To submit your work, please execute all cells of this notebook, save it, and submit it as homework on Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using NLTK to download and tokenize a text\n",
    "\n",
    "Please install NLTK (the Natural Language Processing Toolkit) by following the installation instructions at the [NLTK website](http://www.nltk.org/install.html).  To use NLTK, first `import nltk`.  NLTK has a download manager (try `nltk.download()` in command line) which can import several resources, including corpora.\n",
    "\n",
    "To get started, look at [Chapter 1](http://www.nltk.org/book/ch01.html) of the [online NLTK book (NLP with Python)](http://www.nltk.org/book/) and use the commands there as a model.  <span style=\"color:gray\">Note: the online book was updated for Python 3, but the [printed book](http://shop.oreilly.com/product/9780596516499.do) is only for Python 2.</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt') # execute only once after installing NLTK, then comment it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a.** Using inspiration from [Chapter 3 (3.1. Processing Raw Text) of the NLTK book](http://www.nltk.org/book/ch03.html), download a book from the Gutenberg Project in text format. What is its size? Are these bytes or characters? <span style=\"color:gray\">Note: to learn more about special characters, you can refer to Python's documentation of [Unicode support](https://docs.python.org/3/howto/unicode.html).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request # you may need to run first:  !pip install urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book download complete.\n",
      "Book size in bytes: 1174876\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Book download\n",
    "url = \"https://www.gutenberg.org/ebooks/15960.txt.utf-8\"\n",
    "filename = \"book.txt\"\n",
    "\n",
    "# Download the file\n",
    "response = request.urlopen(url)\n",
    "data = response.read()\n",
    "\n",
    "# Save it locally\n",
    "with open(filename, \"wb\") as f:\n",
    "    f.write(data)\n",
    "\n",
    "print(\"Book download complete.\")\n",
    "\n",
    "# Book file size\n",
    "file_size = os.path.getsize(\"book.txt\")\n",
    "print(\"Book size in bytes:\", file_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b.** We want to keep only the original text of the book, without the header, preface, or license.  Please determine (e.g. by locating the position of initial and final strings) how much your should trim from the beginning and from the end in order to keep only the original text of the book (including titles). Please remove unnecessary paragraph marks (e.g. if the text is segmented into fixed-length lines). Save the result as a new string and display its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed from beginning (lines): 25\n",
      "Trimmed from end (lines): 351\n",
      "Length of cleaned text (characters): 1126609\n",
      "Cleaned file saved as: book_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "input_file = \"book.txt\"\n",
    "output_file = \"book_cleaned.txt\"\n",
    "\n",
    "# Read file as lines\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "start_index = None\n",
    "end_index = None\n",
    "\n",
    "# Find markers\n",
    "for i, line in enumerate(lines):\n",
    "    if \"*** START OF THE PROJECT\" in line and start_index is None:\n",
    "        start_index = i\n",
    "    if \"*** END OF THE PROJECT\" in line:\n",
    "        end_index = i\n",
    "        break\n",
    "\n",
    "if start_index is None or end_index is None or end_index <= start_index:\n",
    "    raise ValueError(\"Could not find valid START/END markers in the file.\")\n",
    "\n",
    "# How much to trim (in lines)\n",
    "trim_begin_lines = start_index + 1\n",
    "trim_end_lines = len(lines) - end_index\n",
    "\n",
    "# Extract main content (exclude marker lines)\n",
    "content = lines[start_index + 1:end_index]\n",
    "\n",
    "# Remove empty lines after START\n",
    "while content and content[0].strip() == \"\":\n",
    "    content.pop(0)\n",
    "\n",
    "# Remove empty lines before END\n",
    "while content and content[-1].strip() == \"\":\n",
    "    content.pop()\n",
    "\n",
    "# Join text\n",
    "cleaned_text = \"\".join(content)\n",
    "\n",
    "# Remove fixed-length line breaks for better NLP tokenization\n",
    "cleaned_text = cleaned_text.replace(\"\\n\", \" \")\n",
    "cleaned_text = \" \".join(cleaned_text.split())\n",
    "\n",
    "print(\"Trimmed from beginning (lines):\", trim_begin_lines)\n",
    "print(\"Trimmed from end (lines):\", trim_end_lines)\n",
    "print(\"Length of cleaned text (characters):\", len(cleaned_text))\n",
    "\n",
    "# Save cleaned text\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_text)\n",
    "\n",
    "print(f\"Cleaned file saved as: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1c.** NLTK defines a function to segment a text into sentences (`nltk.sent_tokenize(...)` (documented [here](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize))) and another one to tokenize a text into words (`nltk.word_tokenize(...)` (documented [here](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize))). <span style=\"color:gray\">Note: NLTK calls the first one \"sentence tokenization\", which is unusual.</span>\n",
    "\n",
    "Please segment the text above into sentences with NLTK, display the number of sentences, and display five sentences of your choice.  Please assess briefly the quality of the segmentation.  If you think that some special characters degrade the results, please go back and remove or replace them in the full text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the fifth time I revise a subject which has occupied my inquiries from early life, with feelings still delightful, and an enthusiasm not wholly diminished.\n",
      "Had not the principle upon which this work is constructed occurred to me in my youth, the materials which illustrate the literary character could never have been brought together.\n",
      "It was in early life that I conceived the idea of pursuing the history of genius by the similar events which had occurred to men of genius.\n",
      "Searching into literary history for the literary character formed a course of experimental philosophy in which every new essay verified a former trial, and confirmed a former truth.\n",
      "By the great philosophical principle of induction, inferences were deduced and results established, which, however vague and doubtful in speculation, are irresistible when the appeal is made to facts as they relate to others, and to feelings which must be decided on as they are passing in our own breast.\n",
      "6811\n"
     ]
    }
   ],
   "source": [
    "sentenc_list = nltk.tokenize.sent_tokenize(cleaned_text)\n",
    "for sentenc in sentenc_list[101:106]:\n",
    "    print(sentenc)\n",
    "print(len(sentenc_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1d.** Please save the resulting text into a file, one sentence per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sentences to file\n",
    "output_file = \"sentences.txt\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in sentenc_list:\n",
    "        f.write(sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1e.** Please segment each sentence into tokens,  store the result in a new variable (a list of lists), and display the same five sentences as above.  Please comment briefly on the quality of the tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For', 'the', 'fifth', 'time', 'I', 'revise', 'a', 'subject', 'which', 'has', 'occupied', 'my', 'inquiries', 'from', 'early', 'life', ',', 'with', 'feelings', 'still', 'delightful', ',', 'and', 'an', 'enthusiasm', 'not', 'wholly', 'diminished', '.']\n",
      "['Had', 'not', 'the', 'principle', 'upon', 'which', 'this', 'work', 'is', 'constructed', 'occurred', 'to', 'me', 'in', 'my', 'youth', ',', 'the', 'materials', 'which', 'illustrate', 'the', 'literary', 'character', 'could', 'never', 'have', 'been', 'brought', 'together', '.']\n",
      "['It', 'was', 'in', 'early', 'life', 'that', 'I', 'conceived', 'the', 'idea', 'of', 'pursuing', 'the', 'history', 'of', 'genius', 'by', 'the', 'similar', 'events', 'which', 'had', 'occurred', 'to', 'men', 'of', 'genius', '.']\n",
      "['Searching', 'into', 'literary', 'history', 'for', 'the', 'literary', 'character', 'formed', 'a', 'course', 'of', 'experimental', 'philosophy', 'in', 'which', 'every', 'new', 'essay', 'verified', 'a', 'former', 'trial', ',', 'and', 'confirmed', 'a', 'former', 'truth', '.']\n",
      "['By', 'the', 'great', 'philosophical', 'principle', 'of', 'induction', ',', 'inferences', 'were', 'deduced', 'and', 'results', 'established', ',', 'which', ',', 'however', 'vague', 'and', 'doubtful', 'in', 'speculation', ',', 'are', 'irresistible', 'when', 'the', 'appeal', 'is', 'made', 'to', 'facts', 'as', 'they', 'relate', 'to', 'others', ',', 'and', 'to', 'feelings', 'which', 'must', 'be', 'decided', 'on', 'as', 'they', 'are', 'passing', 'in', 'our', 'own', 'breast', '.']\n"
     ]
    }
   ],
   "source": [
    "lis_of_lis = []\n",
    "for sentenc in sentenc_list:\n",
    "    worl_sent_list = nltk.tokenize.word_tokenize(sentenc)\n",
    "    lis_of_lis.append(worl_sent_list)\n",
    "\n",
    "for sent_word_list in lis_of_lis[101:106]:\n",
    "    print(sent_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1f.** Please display the total number of tokens found in the text by this method (sentence segmentation followed by sentence-level tokenization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of tokens in the list of lists: 222806\n"
     ]
    }
   ],
   "source": [
    "total_tokens_lisoflis = 0\n",
    "\n",
    "for sentens_word_list in lis_of_lis:\n",
    "    total_tokens_lisoflis += len(sentens_word_list)\n",
    "\n",
    "print(\"Total amount of tokens in the list of lists:\", total_tokens_lisoflis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1g.** Please tokenize now directly the initial full text, without segmenting it into sentences.  Please display the total number of tokens found, and compare this number with the one obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ammount of tokens in the text, via raw text to word tokenizer: 222801\n"
     ]
    }
   ],
   "source": [
    "total_text_tokens = len(nltk.tokenize.word_tokenize(cleaned_text))\n",
    "\n",
    "print(\"Total ammount of tokens in the text, via raw text to word tokenizer:\", total_text_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computing lexical statistics \n",
    "\n",
    "**2a.** Please determine the size of the vocabulary of your text (the number of unique *types*) by converting the list of tokens to a Python `set`.  Note that these *types* include punctuations and other symbols found through tokenization, and that uppercase/lowercase letters are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18515"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_set = set(nltk.tokenize.word_tokenize(cleaned_text))\n",
    "len(vocab_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** What is the type-to-token ratio (TTR) of your text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Type to token ratio of the text is: 0.08310106328068545\n"
     ]
    }
   ],
   "source": [
    "print(\"The Type to token ratio of the text is:\", len(vocab_set)/total_text_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c.** Please create a `nltk.Text` object from the tokenized version of your text, without the sentence segmentation.  Such an object will enable you to compute statistics using NLTK functions.  [Chapter 1 of the NLTK book](http://www.nltk.org/book/ch01.html) provides examples of use.\n",
    "\n",
    "<span style=\"color:gray\">Note: `nltk.word_tokenize()` and `nltk.sent_tokenize()` apply to strings but not directly to `ntlk.Text` objects.  A `nltk.Text` object can store either a string, or a list of words, or a list of sentences (list of lists of strings).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: E-text prepared by Jonathan Ingram , John R....>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltext = nltk.Text(nltk.tokenize.word_tokenize(cleaned_text))\n",
    "nltext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2d.** Please construct the frequency distribution of your text, which is an object of the `nltk.FreqDist`class, instantiated directly from the `nltk.Text` object containing the list of all words. See [Sec. 3.1 of Ch. 1 of the NLTK book](http://www.nltk.org/book/ch01.html#frequency-distributions).  Using the `most_common` method on the `FreqDist` object, find the 50 most frequent words in your text, and display among them the words that have at least 4 characters.  Please comment briefly on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that\n",
      "which\n",
      "with\n",
      "their\n",
      "have\n",
      "this\n",
      "genius\n",
      "from\n",
      "they\n",
      "were\n",
      "great\n"
     ]
    }
   ],
   "source": [
    "voc_distributio = nltk.FreqDist(nltext)\n",
    "for entry in voc_distributio.most_common(50):\n",
    "    if len(entry[0])>3:\n",
    "        print(entry[0])\n",
    "\n",
    "\n",
    "# COMMENT ON THE RESULT (to check)\n",
    "# We see in the resluts words like \"that\", \"which\" and more which war very common words in english, used for gramatical purpuses rather then conveing specific meaning them self.\n",
    "# But we also see words like \"genius\" and \"great\" which do not fall in to the fist category of words, where they do hold greater meaning in the sentenc and are text specific, that they appre so offen, and not like the first category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Zipf's law on a book\n",
    "\n",
    "[Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) formalizes the following empirical observation for large texts or collections: when ranking word types by decreasing frequencies, the ranks *x* (i.e. 1, 2, 3, ...) and the numbers of occurrences *y* (e.g. 948, 321, 146, ...) of each word are related approximately by the formula *y = a / x^b*.  The parameters *a* and *b* depend on the text.  If plotted in log-log coordinates, this relation results in a linear plot (because log(*y*) = *a* - *b* log(*x*)).\n",
    "\n",
    "**3a.** Using the `FreqDist` object, please collect the frequencies of the 1000 most frequent words, rank them by decreasing values, and plot the (*rank*, *frequency*) curve on a log-log scale by setting the `.xscale(\"log\")` and `.yscale(\"log\")` parameters of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Please find the values of *a* and *b* which lead to the closest matching between the observation curve (*rank*, *frequency*) and Zipf's curve (*x*, *y = a / x^b*).  You can use `scipy.optimize.curve_fit()` or even a trial-and-error approach.  Please display both curves on the same graph with a log-log scale to visualize how close they are.  Please also display the optimal values of *a* and *b*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c.** Based on the graph, can you conclude that the frequencies of tokens in the text you select corroborate Zipf's law, or rather contradict it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your answer in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Does an undeciphered manuscript obey Zipf's law?\n",
    "\n",
    "The [Voynich manuscript](https://en.wikipedia.org/wiki/Voynich_manuscript) is an undeciphered manuscript from the 15th century.   Its script and language are still unknown, and it may even be a hoax.  You can read more about it at [Voynich.nu](http://www.voynich.nu/), which provides pictures and transcriptions.  A version of it converted to ASCII characters (corresponding to symbols from the manuscript) and tokenized with one word per line is made available for this lab as `voynich.txt`.\n",
    "\n",
    "**4a.** Please compute the number of tokens, the number of types, and the type-to-token ration (TTR) for this document.  How does it compare with your previous text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.** Considering the 1000 most frequent tokens of the Voynich manuscript, do they follow Zipf's law?  Please also display the optimal values of *a* and *b* and the mean absolute percentage error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4c.** What are your conclusions regarding Voynich's manuscript?  Is it likely to be similar to a real text in an unknown language?  In your answer, consider the values of TTR, the two parameters *a* and *b*, and the fitting of the *y = a / x^b* curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your answer in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing Zipf's law with BPE tokenization of the book and the manuscript\n",
    "\n",
    "**5a.** Please install the SentencePiece tokenizer from https://github.com/google/sentencepiece (with `!pip install sentencepiece`).  Please read the \"Usage instructions‚Äù from the repo, or the ones for the [Python module](https://github.com/google/sentencepiece/blob/master/python/README.md).  Please construct a BPE subword vocabulary (i.e. \"train\" the model) on your text, of size 1100.  Use here the file you saved in (1d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5b.** Please tokenize your text with this model, and display the number of tokens (i.e. size of vocabulary), the number of types, and the type-to-token ratio (TTR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5c.** Please fit a Zipf's curve to the book, as tokenized with BPE, and display the two curves on a log-log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5d.** Please perform the same operations on the Voynich manuscript with a BPE tokenizer build from this text.  Please display the number of tokens, types, TTR, and the fitted Zipf's curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5e.** Please conclude: when using BPE, does the Voynich manuscript exhibit similar properties of token frequency as a real text?  Are the differences larger or smaller when using BPE then when using word-based tokenization ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your answer in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyzing Karpathy's BPE tokenization implementation\n",
    "\n",
    "Please study the following  [minbpe repository](https://github.com/karpathy/minbpe) first and then answer the following questions by indicating the respective file plus codes lines which answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6a.** What pre-processing steps are used before BPE training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your answer in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6b.** What tie-breaking strategy is implemented for pairs with equal count statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your answer in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6c.** Are single characters contained in the final vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your answer in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6d.** How are out-of-vocabulary terms treated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your answer in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of AdvNLP Lab 1\n",
    "Please clean and save the completed notebook, and upload it to Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
